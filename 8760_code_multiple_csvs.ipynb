{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8760 Load Analysis \n",
    "## Purpose\n",
    "The goal of this code is to take 8760 .csv files from electrical systems and analyze when and where the load is not being met. It was specifically designed with the output from the HOMER microgrid software in mind. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from tkinter import Tk, filedialog, Label, Button, Entry, messagebox\n",
    "import seaborn as sns\n",
    "import os \n",
    "import pathlib\n",
    "from glob  import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define directories \n",
    "#load csv file file path \n",
    "dir_path = os.path.join(\n",
    "    pathlib.Path.home(),\n",
    "        'code-projects',\n",
    "        'load-8760'\n",
    "        )\n",
    "\n",
    "# results directory for saving results\n",
    "results_dir = os.path.join(dir_path, 'results')\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "data_files = glob(os.path.join(dir_path,'*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for counting consecutive groups, for counting number of outages per day \n",
    "\n",
    "def count_consecutive_groups(df):\n",
    "    groups = (df['hour'].diff() != 1).cumsum()  # Identify consecutive groups\n",
    "    return len(groups.unique()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for plotting heatmap from days \n",
    "\n",
    "def plot_yearly_heatmap(outages_day):\n",
    "    outages_day['date'] = pd.to_datetime({\n",
    "    'year': 2000,  # Adjust the year if needed\n",
    "    'month': outages_day['month'],\n",
    "    'day': outages_day['day']\n",
    "})\n",
    "\n",
    "    # Create a full-year calendar\n",
    "    full_year = pd.date_range(start='2000-01-01', end='2000-12-31')\n",
    "    full_year_df = pd.DataFrame({'date': full_year})\n",
    "    full_year_df['day_of_week'] = full_year_df['date'].dt.dayofweek\n",
    "    full_year_df['week'] = full_year_df['date'].dt.isocalendar().week\n",
    "\n",
    "    # Merge outages_day into the full year to align with calendar dates\n",
    "    full_year_df = full_year_df.merge(\n",
    "        outages_day[['date', '#_of_outages']],\n",
    "        on='date',\n",
    "        how='left'\n",
    "    )\n",
    "    full_year_df['#_of_outages'] = full_year_df['#_of_outages'].fillna(0)  # Fill missing values with 0\n",
    "\n",
    "    # Aggregate data to ensure unique combinations of week and day_of_week\n",
    "    full_year_df_agg = full_year_df.groupby(['week', 'day_of_week']).agg({'#_of_outages': 'sum'}).reset_index()\n",
    "\n",
    "    # Pivot the data for heat map structure\n",
    "    heatmap_data = full_year_df_agg.pivot(index='week', columns='day_of_week', values='#_of_outages')\n",
    "\n",
    "    #Title Heatmap based on file name\n",
    "    heatmap_title = 'Daily Outages Heat Map for ' + f\"{os.path.basename(data_path)}\" \n",
    "\n",
    "    # Plot the heat map\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(\n",
    "        heatmap_data,\n",
    "        cmap='cool',  # Color palette\n",
    "        linewidths=0.5,   # Grid lines\n",
    "        annot=False,      # Set True if you want annotations\n",
    "        cbar_kws={'label': 'Number of Outages'}\n",
    "    )\n",
    "    plt.title(heatmap_title, fontsize=16)\n",
    "    plt.xlabel('Day of the Week', fontsize=12)\n",
    "    plt.ylabel('Week of the Year', fontsize=12)\n",
    "    plt.xticks(ticks=np.arange(7), labels=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'], rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the heat map\n",
    "    output_path = os.path.join(results_dir, f\"{os.path.basename(data_path)}_heatmap.png\")\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "    return print(f\"Saved heat map for {data_path} to {results_dir}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Customers \n",
    "\n",
    "no_customers = 403 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data[\"Time\"] = pd.to_datetime(data[\"Time\"])\n",
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:56: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  outages_day = unmet_load_df.groupby(['month', 'day']).apply(count_consecutive_groups).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved heat map for C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\10_results_P3.csv to C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data[\"Time\"] = pd.to_datetime(data[\"Time\"])\n",
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:56: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  outages_day = unmet_load_df.groupby(['month', 'day']).apply(count_consecutive_groups).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved heat map for C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\11_results_P3.csv to C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data[\"Time\"] = pd.to_datetime(data[\"Time\"])\n",
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:56: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  outages_day = unmet_load_df.groupby(['month', 'day']).apply(count_consecutive_groups).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved heat map for C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\12_results_P3.csv to C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data[\"Time\"] = pd.to_datetime(data[\"Time\"])\n",
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:56: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  outages_day = unmet_load_df.groupby(['month', 'day']).apply(count_consecutive_groups).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved heat map for C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\13_results_P3.csv to C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data[\"Time\"] = pd.to_datetime(data[\"Time\"])\n",
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:56: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  outages_day = unmet_load_df.groupby(['month', 'day']).apply(count_consecutive_groups).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved heat map for C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\14_results_P3.csv to C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data[\"Time\"] = pd.to_datetime(data[\"Time\"])\n",
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:56: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  outages_day = unmet_load_df.groupby(['month', 'day']).apply(count_consecutive_groups).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved heat map for C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\15_results_P3.csv to C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data[\"Time\"] = pd.to_datetime(data[\"Time\"])\n",
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:56: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  outages_day = unmet_load_df.groupby(['month', 'day']).apply(count_consecutive_groups).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved heat map for C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\16_results_P3.csv to C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data[\"Time\"] = pd.to_datetime(data[\"Time\"])\n",
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:56: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  outages_day = unmet_load_df.groupby(['month', 'day']).apply(count_consecutive_groups).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved heat map for C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\17_results_P3.csv to C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data[\"Time\"] = pd.to_datetime(data[\"Time\"])\n",
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:56: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  outages_day = unmet_load_df.groupby(['month', 'day']).apply(count_consecutive_groups).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved heat map for C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\18_results_P3.csv to C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data[\"Time\"] = pd.to_datetime(data[\"Time\"])\n",
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:56: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  outages_day = unmet_load_df.groupby(['month', 'day']).apply(count_consecutive_groups).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved heat map for C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\19_results_P3.csv to C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data[\"Time\"] = pd.to_datetime(data[\"Time\"])\n",
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:56: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  outages_day = unmet_load_df.groupby(['month', 'day']).apply(count_consecutive_groups).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved heat map for C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\1_results_NO.EC.csv to C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data[\"Time\"] = pd.to_datetime(data[\"Time\"])\n",
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:56: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  outages_day = unmet_load_df.groupby(['month', 'day']).apply(count_consecutive_groups).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved heat map for C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\20_results_P3.csv to C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data[\"Time\"] = pd.to_datetime(data[\"Time\"])\n",
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:56: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  outages_day = unmet_load_df.groupby(['month', 'day']).apply(count_consecutive_groups).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved heat map for C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\2_results.NO EC.csv to C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data[\"Time\"] = pd.to_datetime(data[\"Time\"])\n",
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:56: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  outages_day = unmet_load_df.groupby(['month', 'day']).apply(count_consecutive_groups).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved heat map for C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\3_results NO EC.csv to C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data[\"Time\"] = pd.to_datetime(data[\"Time\"])\n",
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:56: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  outages_day = unmet_load_df.groupby(['month', 'day']).apply(count_consecutive_groups).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved heat map for C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\4_results NO EC.csv to C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data[\"Time\"] = pd.to_datetime(data[\"Time\"])\n",
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:56: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  outages_day = unmet_load_df.groupby(['month', 'day']).apply(count_consecutive_groups).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved heat map for C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\5_results NO EC.csv to C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data[\"Time\"] = pd.to_datetime(data[\"Time\"])\n",
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:56: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  outages_day = unmet_load_df.groupby(['month', 'day']).apply(count_consecutive_groups).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved heat map for C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\6_results_P3.csv to C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data[\"Time\"] = pd.to_datetime(data[\"Time\"])\n",
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:56: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  outages_day = unmet_load_df.groupby(['month', 'day']).apply(count_consecutive_groups).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved heat map for C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\7_results_P3.csv to C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data[\"Time\"] = pd.to_datetime(data[\"Time\"])\n",
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:56: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  outages_day = unmet_load_df.groupby(['month', 'day']).apply(count_consecutive_groups).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved heat map for C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\8_results_P3.csv to C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data[\"Time\"] = pd.to_datetime(data[\"Time\"])\n",
      "C:\\Users\\Nolan Welsh\\AppData\\Local\\Temp\\ipykernel_32252\\2921703538.py:56: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  outages_day = unmet_load_df.groupby(['month', 'day']).apply(count_consecutive_groups).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved heat map for C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\9_results_P3.csv to C:\\Users\\Nolan Welsh\\code-projects\\load-8760\\results\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#list to store all results\n",
    "\n",
    "results = []\n",
    "outages_summary=[]\n",
    "\n",
    "\n",
    "for data_path in data_files:\n",
    "    data = pd.read_csv(\n",
    "        data_path,\n",
    "        delimiter=',',\n",
    "        header= 0,\n",
    "        index_col='Time',\n",
    "        skiprows=1)\n",
    "     \n",
    "    data= data[data.index.notnull()]\n",
    "\n",
    "    data=data.reset_index() #Reset Index\n",
    "    data[\"Time\"] = pd.to_datetime(data[\"Time\"])\n",
    "    data[\"hour\"] = data['Time'].map(lambda x: x.hour)\n",
    "    data[\"day\"] = data['Time'].map(lambda x: x.day)\n",
    "    data[\"month\"] = data['Time'].map(lambda x: x.month)\n",
    "    \n",
    "\n",
    "     #Initialize N/A\n",
    "    hours_shortage = '0'\n",
    "    hours_battery_shortage = '0'\n",
    "    hours_unmet_load = '0' \n",
    "    num_days_outages = '0' \n",
    "    outages_year = '0' \n",
    "    # filter  how many hours load not met (capacity shortage > 0)\n",
    "    if 'Capacity Shortage' in data.columns:\n",
    "        data['Capacity Shortage'] = data['Capacity Shortage'].astype(float) #first need to force column to convert to float \n",
    "        capacity_shortage_df = data[data['Capacity Shortage'] > 0 ]\n",
    "        hours_shortage = len(capacity_shortage_df)\n",
    "   \n",
    "\n",
    "    #Calculate how many hours annual load is unmet   #for Battery <30%\n",
    "    if 'Generic 1kWh Li-Ion State of Charge' in data.columns:\n",
    "        data['Generic 1kWh Li-Ion State of Charge'] = data['Generic 1kWh Li-Ion State of Charge'].astype(float)\n",
    "        battery_shortage_df = data[data['Generic 1kWh Li-Ion State of Charge'] < 30 ]\n",
    "        hours_battery_shortage = len(battery_shortage_df)\n",
    "\n",
    "    # To find hours with Unmet Electrical Load\n",
    "    if 'Unmet Electrical Load' in data.columns:\n",
    "        data['Unmet Electrical Load'] = data['Unmet Electrical Load'].astype(float)\n",
    "        unmet_load_df = data[data['Unmet Electrical Load'] > 0.01 ]\n",
    "        hours_unmet_load = len(unmet_load_df)\n",
    "    \n",
    "    hours_per_day = unmet_load_df.groupby(['month','day']).size().reset_index()\n",
    "    hours_per_day.columns=['month','day','hours_with_unmet_load']\n",
    "    #Calculate number of days with outages \n",
    "    num_days_outages = len(hours_per_day)\n",
    "    # Calulate number of outages per year\n",
    "    outages_year = count_consecutive_groups (unmet_load_df)\n",
    "    #Calculate number of distinct outages \n",
    "    outages_day = unmet_load_df.groupby(['month', 'day']).apply(count_consecutive_groups).reset_index()\n",
    "    outages_day.columns = ['month','day','#_of_outages']\n",
    "\n",
    "\n",
    "    #save heatmap for each file\n",
    "    plot_yearly_heatmap(outages_day)\n",
    "\n",
    "    #complile a list of results for summary \n",
    "    results.append ({\n",
    "         'File': os.path.basename(data_path),\n",
    "        'Hours with Capacity Shortage': hours_shortage,\n",
    "        'Hours with Battery <30%': hours_battery_shortage,\n",
    "        'SAIDI (hrs outages/year)': hours_unmet_load,\n",
    "        'Days with Outages': num_days_outages,\n",
    "        'SAIFI (outages/year)': outages_year,\n",
    "      \n",
    "    })\n",
    "\n",
    "    #Merge hours_per_day and outages_day data frames\n",
    "    if not hours_per_day.empty and not outages_day.empty:\n",
    "        merged = pd.merge(hours_per_day, outages_day, on=['month', 'day'], how='outer')\n",
    "        merged['File'] = os.path.basename(data_path)\n",
    "        outages_summary.append(merged)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "outages_summary_df = pd.concat(outages_summary, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Hours with Capacity Shortage</th>\n",
       "      <th>Hours with Battery &lt;30%</th>\n",
       "      <th>SAIDI (hrs outages/year)</th>\n",
       "      <th>Days with Outages</th>\n",
       "      <th>SAIFI (outages/year)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_results_P3.csv</td>\n",
       "      <td>1812</td>\n",
       "      <td>2128</td>\n",
       "      <td>1316</td>\n",
       "      <td>186</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11_results_P3.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12_results_P3.csv</td>\n",
       "      <td>104</td>\n",
       "      <td>153</td>\n",
       "      <td>84</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13_results_P3.csv</td>\n",
       "      <td>190</td>\n",
       "      <td>265</td>\n",
       "      <td>143</td>\n",
       "      <td>18</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14_results_P3.csv</td>\n",
       "      <td>1056</td>\n",
       "      <td>1206</td>\n",
       "      <td>684</td>\n",
       "      <td>119</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15_results_P3.csv</td>\n",
       "      <td>1836</td>\n",
       "      <td>2158</td>\n",
       "      <td>1241</td>\n",
       "      <td>190</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16_results_P3.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17_results_P3.csv</td>\n",
       "      <td>104</td>\n",
       "      <td>160</td>\n",
       "      <td>71</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>18_results_P3.csv</td>\n",
       "      <td>202</td>\n",
       "      <td>277</td>\n",
       "      <td>135</td>\n",
       "      <td>23</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19_results_P3.csv</td>\n",
       "      <td>963</td>\n",
       "      <td>1051</td>\n",
       "      <td>573</td>\n",
       "      <td>104</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1_results_NO.EC.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20_results_P3.csv</td>\n",
       "      <td>1912</td>\n",
       "      <td>1806</td>\n",
       "      <td>1072</td>\n",
       "      <td>175</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2_results.NO EC.csv</td>\n",
       "      <td>92</td>\n",
       "      <td>138</td>\n",
       "      <td>77</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3_results NO EC.csv</td>\n",
       "      <td>162</td>\n",
       "      <td>218</td>\n",
       "      <td>136</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4_results NO EC.csv</td>\n",
       "      <td>830</td>\n",
       "      <td>1043</td>\n",
       "      <td>639</td>\n",
       "      <td>92</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5_results NO EC.csv</td>\n",
       "      <td>2033</td>\n",
       "      <td>2004</td>\n",
       "      <td>1345</td>\n",
       "      <td>197</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6_results_P3.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7_results_P3.csv</td>\n",
       "      <td>103</td>\n",
       "      <td>124</td>\n",
       "      <td>84</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8_results_P3.csv</td>\n",
       "      <td>174</td>\n",
       "      <td>237</td>\n",
       "      <td>138</td>\n",
       "      <td>16</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9_results_P3.csv</td>\n",
       "      <td>938</td>\n",
       "      <td>1225</td>\n",
       "      <td>690</td>\n",
       "      <td>111</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   File Hours with Capacity Shortage  Hours with Battery <30%  \\\n",
       "0     10_results_P3.csv                         1812                     2128   \n",
       "1     11_results_P3.csv                            0                       57   \n",
       "2     12_results_P3.csv                          104                      153   \n",
       "3     13_results_P3.csv                          190                      265   \n",
       "4     14_results_P3.csv                         1056                     1206   \n",
       "5     15_results_P3.csv                         1836                     2158   \n",
       "6     16_results_P3.csv                            0                       33   \n",
       "7     17_results_P3.csv                          104                      160   \n",
       "8     18_results_P3.csv                          202                      277   \n",
       "9     19_results_P3.csv                          963                     1051   \n",
       "10  1_results_NO.EC.csv                            0                       47   \n",
       "11    20_results_P3.csv                         1912                     1806   \n",
       "12  2_results.NO EC.csv                           92                      138   \n",
       "13  3_results NO EC.csv                          162                      218   \n",
       "14  4_results NO EC.csv                          830                     1043   \n",
       "15  5_results NO EC.csv                         2033                     2004   \n",
       "16     6_results_P3.csv                            0                       52   \n",
       "17     7_results_P3.csv                          103                      124   \n",
       "18     8_results_P3.csv                          174                      237   \n",
       "19     9_results_P3.csv                          938                     1225   \n",
       "\n",
       "    SAIDI (hrs outages/year)  Days with Outages  SAIFI (outages/year)  \n",
       "0                       1316                186                   252  \n",
       "1                         14                  4                     4  \n",
       "2                         84                 11                    14  \n",
       "3                        143                 18                    25  \n",
       "4                        684                119                   154  \n",
       "5                       1241                190                   248  \n",
       "6                         13                  3                     2  \n",
       "7                         71                 10                    13  \n",
       "8                        135                 23                    27  \n",
       "9                        573                104                   137  \n",
       "10                        14                  3                     3  \n",
       "11                      1072                175                   252  \n",
       "12                        77                 10                    15  \n",
       "13                       136                 17                    23  \n",
       "14                       639                 92                   117  \n",
       "15                      1345                197                   314  \n",
       "16                        13                  4                     6  \n",
       "17                        84                 10                    16  \n",
       "18                       138                 16                    22  \n",
       "19                       690                111                   135  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hours_with_unmet_load</th>\n",
       "      <th>#_of_outages</th>\n",
       "      <th>date</th>\n",
       "      <th>File</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2000-01-08</td>\n",
       "      <td>10_results_P3.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000-01-12</td>\n",
       "      <td>10_results_P3.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2000-01-13</td>\n",
       "      <td>10_results_P3.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2000-01-19</td>\n",
       "      <td>10_results_P3.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2000-01-20</td>\n",
       "      <td>10_results_P3.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2000-12-18</td>\n",
       "      <td>9_results_P3.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>2000-12-19</td>\n",
       "      <td>9_results_P3.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2000-12-20</td>\n",
       "      <td>9_results_P3.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000-12-30</td>\n",
       "      <td>9_results_P3.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302</th>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2000-12-31</td>\n",
       "      <td>9_results_P3.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1303 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      month  day  hours_with_unmet_load  #_of_outages       date  \\\n",
       "0         1    8                      4             1 2000-01-08   \n",
       "1         1   12                      1             1 2000-01-12   \n",
       "2         1   13                     10             1 2000-01-13   \n",
       "3         1   19                      6             1 2000-01-19   \n",
       "4         1   20                      8             1 2000-01-20   \n",
       "...     ...  ...                    ...           ...        ...   \n",
       "1298     12   18                     11             1 2000-12-18   \n",
       "1299     12   19                     22             3 2000-12-19   \n",
       "1300     12   20                      7             1 2000-12-20   \n",
       "1301     12   30                      1             1 2000-12-30   \n",
       "1302     12   31                      9             1 2000-12-31   \n",
       "\n",
       "                   File  \n",
       "0     10_results_P3.csv  \n",
       "1     10_results_P3.csv  \n",
       "2     10_results_P3.csv  \n",
       "3     10_results_P3.csv  \n",
       "4     10_results_P3.csv  \n",
       "...                 ...  \n",
       "1298   9_results_P3.csv  \n",
       "1299   9_results_P3.csv  \n",
       "1300   9_results_P3.csv  \n",
       "1301   9_results_P3.csv  \n",
       "1302   9_results_P3.csv  \n",
       "\n",
       "[1303 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outages_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>#_of_outages</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2000-01-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>2000-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>2000-01-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2000-02-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2000-02-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>2000-12-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>2000-12-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>2000-12-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>2000-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>2000-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     month  day  #_of_outages       date\n",
       "0        1   13             1 2000-01-13\n",
       "1        1   19             1 2000-01-19\n",
       "2        1   20             1 2000-01-20\n",
       "3        2    3             1 2000-02-03\n",
       "4        2    4             1 2000-02-04\n",
       "..     ...  ...           ...        ...\n",
       "106     12   18             1 2000-12-18\n",
       "107     12   19             3 2000-12-19\n",
       "108     12   20             1 2000-12-20\n",
       "109     12   30             1 2000-12-30\n",
       "110     12   31             1 2000-12-31\n",
       "\n",
       "[111 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outages_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Summary  results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summary_path = os.path.join(results_dir, 'outages_summary_results.csv')\n",
    "results_df.to_csv(summary_path, index=False)\n",
    "\n",
    "outages_daily_detail_path = os.path.join(results_dir,'outages_daily_details.csv')\n",
    "outages_summary_df.to_csv(outages_daily_detail_path, index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earth-analytics-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
